{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trxWCT6kLBRM"
      },
      "source": [
        "# Reimplementation: CycleGAN (2017)\n",
        "\n",
        "This implementation focuses on unpaired image-to-image translation of photos to Monet paintings and vice versa. The code for the CycleGAN was built in reference to [this implementation](https://github.com/LynnHo/CycleGAN-Tensorflow-2), as well as the [original CycleGAN implementation](https://junyanz.github.io/CycleGAN/). This notebook has been executed using Colab Pro for the present project. It is possible that (1) this notebook does not run using the free Colab version due to usage constraints, or (2) training time might be increased."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCm69m8fL90u"
      },
      "source": [
        "## 1 Load the data and prepare the notebook\n",
        "\n",
        "The dataset is taken from [here](https://www.kaggle.com/competitions/gan-getting-started/data). This dataset consists of 300 images of Monet paintings with 256 x 256 pixels. There are also 7028 content images of the same size as the Monet paintings.\n",
        "\n",
        "The dataset is loaded into the local Colab storage from the drive of the user of this notebook. For this code to work, the user must have the dataset uploaded to their drive.\n",
        "\n",
        "To use the trained model, one can also load the checkpoints (after 200 epochs) into the local storage. They will automatically be used once loaded."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Connect to Drive\n",
        "This notebook uses the data that is uploaded to a drive in a zip file. This is not openly accessible. If you would like to use the dataset, download it from [here](https://www.kaggle.com/competitions/gan-getting-started/data) and store it in a zip file named data.zip in your Google drive. When extracted, the folder should contain a folder *'monet'* with a subfolder *'monet_jpg'* that contains all images of Monet paintings, as well as a folder *'photos'* with a subfolder *'photos_jpg'* that contains all photographs. All pictures should be of the size 256 x 256 pixels and in jpg format."
      ],
      "metadata": {
        "id": "9L1ZDoqKEriZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "M-tyO6asf-fE"
      },
      "outputs": [],
      "source": [
        "# Connect to Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3y-mrq4ej4K3"
      },
      "outputs": [],
      "source": [
        "# Get the dataset by unzipping from drive to local Colab storage\n",
        "!unzip '/content/gdrive/MyDrive/data.zip' -d '/content/'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Load the latest checkpoint\n",
        "If you have a checkpoint from an earlier training saved in your drive, you can also load it here."
      ],
      "metadata": {
        "id": "x0fmll7KFegB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncXVHt8iFniW"
      },
      "outputs": [],
      "source": [
        "# Get the checkpoint for the trained model\n",
        "# !unzip '/content/gdrive/MyDrive/tf_checkpoints-final.zip' -d '/content/'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Installations and imports"
      ],
      "metadata": {
        "id": "9mbehhG6F8z1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VuZG-uslDqw",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# install the missing library (needed for instance normalization layer)\n",
        "!pip install tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQt5j90oLhZE",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-fid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcY-MRybkIjf"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.optimizers import schedules\n",
        "from keras import Model\n",
        "import keras.layers as layers\n",
        "import tensorflow_addons as tfa\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time as t\n",
        "import os\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZKMmJlvN9Jb"
      },
      "source": [
        "## 2 The Models\n",
        "\n",
        "The CycleGAN consists of a generator and a discriminator. The generator makes use of residual blocks, hence also a Resnet is implemented. The generator also needs down- and upsampling blocks, which are implemented as separate networks to increase understandability of the network structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2zHTx0YOo69"
      },
      "source": [
        "### 2.1 Helper networks: Resnet, Downsampler, and Upsampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6M8-do4ukfSZ"
      },
      "outputs": [],
      "source": [
        "class Resnet(Model):\n",
        "\n",
        "    def __init__(self, dim):\n",
        "        \"\"\"\n",
        "        A Resnet block for CycleGAN consists of a convolution, followed by \n",
        "        normalization, relu activation, another convolution and normalization. \n",
        "        The input and output images are added to create the residual \n",
        "        characteristic.\n",
        "\n",
        "        dim: number of filters\n",
        "        \"\"\"\n",
        "        super(Resnet, self).__init__()\n",
        "\n",
        "        self.res_conv1 = layers.Conv2D(filters=dim, \n",
        "                                       kernel_size=3, \n",
        "                                       padding='valid', \n",
        "                                       use_bias=False)\n",
        "        self.res_norm1 = tfa.layers.InstanceNormalization()\n",
        "        self.res_relu = layers.ReLU()\n",
        "\n",
        "        self.res_conv2 = layers.Conv2D(filters=dim, \n",
        "                                       kernel_size=3, \n",
        "                                       padding='valid', \n",
        "                                       use_bias=False)\n",
        "        self.res_norm2 = tfa.layers.InstanceNormalization()\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, input, training=None):\n",
        "        \"\"\"\n",
        "        Training function for the residual block.\n",
        "\n",
        "        input: input images with dimensions 256 x 256\n",
        "        training: training flag - whether the network is training or not\n",
        "        \"\"\"\n",
        "\n",
        "        x = tf.pad(input, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='REFLECT')\n",
        "        x = self.res_conv1(x)\n",
        "        x = self.res_norm1(x)\n",
        "        x = self.res_relu(x)\n",
        "        x = tf.pad(x, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='REFLECT')\n",
        "        x = self.res_conv2(x)\n",
        "        x = self.res_norm2(x)\n",
        "\n",
        "        return layers.add([input, x])\n",
        "\n",
        "\n",
        "class Downsampler(Model):\n",
        "\n",
        "    def __init__(self, dim, kernel_size=3, strides=2, leaky=False):\n",
        "        \"\"\"\n",
        "        A downsampler block downsamples an image to half its size in height and \n",
        "        width but with a given number of dimensions for the third dimension. A \n",
        "        downsampler block consists of a convolutional layer, instance \n",
        "        normalization, followed by either leaky relu or relu as activation.\n",
        "\n",
        "        dim: number of filters for convolution\n",
        "        kernel_size: kernel size for convolution\n",
        "        strides: strides for convolution\n",
        "        leaky: whether to apply leaky relu or relu as activation\n",
        "        \"\"\"\n",
        "        super(Downsampler, self).__init__()\n",
        "\n",
        "        self.down_conv = layers.Conv2D(filters=dim, \n",
        "                                       kernel_size=kernel_size, \n",
        "                                       strides=strides, \n",
        "                                       padding='same', \n",
        "                                       use_bias=False)\n",
        "        self.down_norm = tfa.layers.InstanceNormalization()\n",
        "        if leaky:\n",
        "          self.down_relu = layers.LeakyReLU()\n",
        "        else:\n",
        "          self.down_relu = layers.ReLU()\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, input):\n",
        "        \"\"\"\n",
        "        Call function for the downsampler blocks.\n",
        "\n",
        "        input: input image with varying number of dimensions\n",
        "        \"\"\"\n",
        "\n",
        "        x = self.down_conv(input)\n",
        "        x = self.down_norm(x)\n",
        "        x = self.down_relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Upsampler(Model):\n",
        "\n",
        "    def __init__(self, dim):\n",
        "        \"\"\"\n",
        "        An upsampler block upsamples a given image to double its height and \n",
        "        width. An upsampler block consists of a transposed convolution, \n",
        "        instance normalization, followed by relu activation.\n",
        "\n",
        "        dim: number of filters for convolution\n",
        "        \"\"\"\n",
        "        super(Upsampler, self).__init__()\n",
        "\n",
        "        self.up_convtp = layers.Conv2DTranspose(filters=dim, \n",
        "                                                kernel_size=3, \n",
        "                                                strides=2, \n",
        "                                                padding='same', \n",
        "                                                use_bias=False)\n",
        "        self.up_norm = tfa.layers.InstanceNormalization()\n",
        "        self.up_relu = layers.ReLU()\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, input):\n",
        "        \"\"\"\n",
        "        Call function for the upsampler blocks.\n",
        "\n",
        "        input: input image with varying number of dimensions\n",
        "        \"\"\"\n",
        "\n",
        "        x = self.up_convtp(input)\n",
        "        x = self.up_norm(x)\n",
        "        x = self.up_relu(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZFUcqO4OwXj"
      },
      "source": [
        "### 2.2 The Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SCZtWSNOirJ"
      },
      "outputs": [],
      "source": [
        "class ResnetGenerator(Model):\n",
        "\n",
        "    def __init__(self, \n",
        "                 inp_shape=(256, 256, 3), \n",
        "                 output_channels=3):\n",
        "        \"\"\"\n",
        "        This Resnet generator consists of a convolution followed by instance \n",
        "        normalization, and relu activation, three downsampler blocks, followed \n",
        "        by seven resnet/ residual blocks, again followed by three upsampling \n",
        "        blocks, and finalized with a convolutional layer to reduce the number \n",
        "        of channels to three, and tanh activation.\n",
        "\n",
        "        inp_shape: the input shape of input images to the network\n",
        "        output_channels: the number of dimensions for the output images\n",
        "        \"\"\"\n",
        "        super(ResnetGenerator, self).__init__()\n",
        "\n",
        "        self.input_layer = layers.Input(shape=inp_shape)\n",
        "        self.block1_conv = layers.Conv2D(filters=64, \n",
        "                                         kernel_size=7, \n",
        "                                         padding='valid', \n",
        "                                         use_bias=False)\n",
        "        self.block1_norm = tfa.layers.InstanceNormalization()\n",
        "        self.block1_relu = layers.ReLU()\n",
        "\n",
        "        self.block2_down1 = Downsampler(dim=128)\n",
        "        self.block2_down2 = Downsampler(dim=256)\n",
        "        self.block2_down3 = Downsampler(dim=512)\n",
        "\n",
        "        self.block3_resnet1 = Resnet(dim=512)\n",
        "        self.block3_resnet2 = Resnet(dim=512)\n",
        "        self.block3_resnet3 = Resnet(dim=512)\n",
        "        self.block3_resnet4 = Resnet(dim=512)\n",
        "        self.block3_resnet5 = Resnet(dim=512)\n",
        "        self.block3_resnet6 = Resnet(dim=512)\n",
        "        self.block3_resnet7 = Resnet(dim=512)\n",
        "        #self.block3_resnet8 = Resnet(dim=512)\n",
        "        #self.block3_resnet9 = Resnet(dim=512)\n",
        "\n",
        "        self.block4_up1_1 = Upsampler(dim=256)\n",
        "        self.block4_up1 = Upsampler(dim=128)\n",
        "        self.block4_up2 = Upsampler(dim=64)\n",
        "\n",
        "        self.out_conv = layers.Conv2D(output_channels, \n",
        "                                      kernel_size=7, \n",
        "                                      padding='valid')\n",
        "        self.out_tanh = layers.Activation(tf.nn.tanh)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, input, training=False):\n",
        "        \"\"\"\n",
        "        Call function for the ResnetGenerator.\n",
        "        \n",
        "        input: a 256 x 256 pixel input image\n",
        "        training: training flag - whether the network is training or not\n",
        "        \"\"\"\n",
        "\n",
        "        #x = self.input_layer(input)\n",
        "        x = tf.pad(input, [[0, 0], [3, 3], [3, 3], [0, 0]], mode='REFLECT')\n",
        "        x = self.block1_conv(x)\n",
        "        x = self.block1_norm(x)\n",
        "        x = self.block1_relu(x)\n",
        "\n",
        "        x = self.block2_down1(x)\n",
        "        x = self.block2_down2(x)\n",
        "        x = self.block2_down3(x)\n",
        "\n",
        "        x = self.block3_resnet1(x)\n",
        "        x = self.block3_resnet2(x)\n",
        "        x = self.block3_resnet3(x)\n",
        "        x = self.block3_resnet4(x)\n",
        "        x = self.block3_resnet5(x)\n",
        "        x = self.block3_resnet6(x)\n",
        "        x = self.block3_resnet7(x)\n",
        "        #x = self.block3_resnet8(x)\n",
        "        #x = self.block3_resnet9(x)\n",
        "\n",
        "        x = self.block4_up1_1(x)\n",
        "        x = self.block4_up1(x)\n",
        "        x = self.block4_up2(x)\n",
        "\n",
        "        x = tf.pad(x, [[0, 0], [3, 3], [3, 3], [0, 0]], mode='REFLECT')\n",
        "        x = self.out_conv(x)\n",
        "        x = self.out_tanh(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t13KzyuyOzB1"
      },
      "source": [
        "### 2.3 The Discriminator\n",
        "\n",
        "The discriminator is a PatchDiscriminator, i.e. it determines for multiple patches in an image, whether the respective patch is real or fake."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlLsNAhYkcJk"
      },
      "outputs": [],
      "source": [
        "class Discriminator(Model):\n",
        "\n",
        "    def __init__(self, inp_shape=(256, 256, 3)):\n",
        "        \"\"\"\n",
        "        The PatchDiscriminator takes an input image and processes it with a \n",
        "        convolutional layer, and leaky relu activation, followed by three \n",
        "        downsampling blocks with relu activation each, and a final output \n",
        "        convolutional layer.\n",
        "\n",
        "        inp_shape: image input shape\n",
        "        \"\"\"\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.input_layer = keras.Input(shape=inp_shape)\n",
        "        self.conv_1 = layers.Conv2D(filters=64,\n",
        "                                    kernel_size=4,\n",
        "                                    strides=2,\n",
        "                                    input_shape=inp_shape,\n",
        "                                    padding='same',\n",
        "                                    activation=None)\n",
        "        self.leaky_relu_1 = layers.LeakyReLU()\n",
        "\n",
        "        self.down_1 = Downsampler(dim=128, kernel_size=4, leaky=True)\n",
        "        self.down_2 = Downsampler(dim=256, kernel_size=4, leaky=True)\n",
        "        self.down_3 = Downsampler(dim=512, kernel_size=4, strides=1, leaky=True)\n",
        "\n",
        "        self.out = layers.Conv2D(filters=1,\n",
        "                                    kernel_size=4,\n",
        "                                    strides=1,\n",
        "                                    padding='same',\n",
        "                                    activation='sigmoid')\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, input, training=False):\n",
        "        \"\"\"\n",
        "        Call function for the PatchDiscriminator\n",
        "        \n",
        "        input: input image\n",
        "        training: training flag - whether the network is training or not\n",
        "        \"\"\"\n",
        "\n",
        "        x = self.conv_1(input)\n",
        "        x = self.leaky_relu_1(x)\n",
        "\n",
        "        x = self.down_1(x)\n",
        "        x = self.down_2(x)\n",
        "        x = self.down_3(x)\n",
        "\n",
        "        x = self.out(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ4_rvjAO_BQ"
      },
      "source": [
        "## 3 Training the network"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Training functions"
      ],
      "metadata": {
        "id": "Vpw1UDfpkzLb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7BfnJk67Wa5"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGrp2ZN5kpWV"
      },
      "outputs": [],
      "source": [
        "def norm(img):\n",
        "    \"\"\"\n",
        "    Normalizes a given image to the range -1 to 1\n",
        "\n",
        "    img: the image to process\n",
        "    \"\"\"\n",
        "    return (img / 255.) * 2 - 1\n",
        "\n",
        "def post(img):\n",
        "    \"\"\"\n",
        "    Postprocesses an image from the range -1 to 1 to the range 0 to 1.\n",
        "\n",
        "    img: the image to process\n",
        "    \"\"\"\n",
        "    return (img * 0.5 + 0.5)\n",
        "\n",
        "@tf.function\n",
        "def train_generator(photo, monet, cycle_loss_weight=10, identity_loss_weight=5):\n",
        "    \"\"\"\n",
        "    Generator training step with loss computation.\n",
        "\n",
        "    :param photo: a photo from dataset\n",
        "    :param monet: a monet image from dataset\n",
        "    :param cycle_loss_weight: weight for cycle consistency loss\n",
        "    :param identity_loss_weight: weight for identity loss\n",
        "    \"\"\"\n",
        "\n",
        "    mse = tf.losses.MeanSquaredError()\n",
        "    bce = tf.losses.BinaryCrossentropy(from_logits=True)\n",
        "    mae = tf.losses.MeanAbsoluteError()\n",
        "\n",
        "    with tf.GradientTape() as t:\n",
        "\n",
        "        P2M = GEN_P2M(photo, training=True) # from photo to monet\n",
        "        M2P = GEN_M2P(monet, training=True) # from monet to photo\n",
        "        P2M2P = GEN_M2P(P2M, training=True) # from photo to monet back to photo\n",
        "        M2P2M = GEN_P2M(M2P, training=True) # from monet to photo to monet\n",
        "        P2P = GEN_M2P(photo, training=True) # photo to photo\n",
        "        M2M = GEN_P2M(monet, training=True) # monet to monet\n",
        "\n",
        "        x = DISC_M(P2M, training=True)\n",
        "        y = DISC_P(M2P, training=True)\n",
        "\n",
        "        disc_loss_P2M = bce(tf.ones_like(x), x)\n",
        "        disc_loss_M2P = bce(tf.ones_like(y), y)\n",
        "\n",
        "        cycle_loss_P2M2P = mae(photo, P2M2P) \n",
        "        cycle_loss_M2P2M = mae(monet, M2P2M)\n",
        "\n",
        "        identity_loss_P2P = mae(photo, P2P)\n",
        "        identity_loss_M2M = mae(monet, M2M)\n",
        "\n",
        "        generator_loss = (cycle_loss_P2M2P + cycle_loss_M2P2M) * cycle_loss_weight + \\\n",
        "                         (identity_loss_P2P + identity_loss_M2M) * identity_loss_weight + \\\n",
        "                         (disc_loss_P2M + disc_loss_M2P)\n",
        "\n",
        "    # Update gradients\n",
        "    gradients = t.gradient(generator_loss, \n",
        "                           GEN_P2M.trainable_variables + GEN_M2P.trainable_variables)\n",
        "    GEN_ADAM.apply_gradients(zip(gradients, \n",
        "                                 GEN_P2M.trainable_variables + GEN_M2P.trainable_variables))\n",
        "\n",
        "    return P2M, M2P, generator_loss\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_discriminator(photo, monet, p2m, m2p):\n",
        "    \"\"\"\n",
        "    Discriminator training step with loss computation.\n",
        "\n",
        "    :param photo: a photo from dataset\n",
        "    :param monet: a monet image from dataset\n",
        "    :param P2M: a generated monet image\n",
        "    :param M2P: a generated photo\n",
        "    \"\"\"\n",
        "\n",
        "    mse = tf.losses.MeanSquaredError()\n",
        "    bce = tf.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "    with tf.GradientTape() as t:\n",
        "        orig_photo = DISC_P(photo, training=True)\n",
        "        orig_monet = DISC_M(monet, training=True)\n",
        "        false_photo = DISC_P(m2p, training=True)\n",
        "        false_monet = DISC_M(p2m, training=True)\n",
        "\n",
        "        orig_photo_loss = bce(tf.ones_like(orig_photo), orig_photo)\n",
        "        false_photo_loss = bce(tf.zeros_like(false_photo), false_photo) * 0.5\n",
        "\n",
        "        orig_monet_loss = bce(tf.ones_like(orig_monet), orig_monet)\n",
        "        false_monet_loss = bce(tf.zeros_like(false_monet), false_monet) * 0.5\n",
        "\n",
        "        discriminator_loss = ((orig_photo_loss + false_photo_loss) + (orig_monet_loss + false_monet_loss)) * 0.5\n",
        "\n",
        "    gradients = t.gradient(discriminator_loss, \n",
        "                           DISC_P.trainable_variables + DISC_M.trainable_variables)\n",
        "    DISC_ADAM.apply_gradients(zip(gradients, \n",
        "                                  DISC_P.trainable_variables + DISC_M.trainable_variables))\n",
        "\n",
        "    return discriminator_loss\n",
        "\n",
        "\n",
        "def train_single_step(photo, monet):\n",
        "    \"\"\"\n",
        "    Function to combine training of generator and discriminator\n",
        "\n",
        "    :param photo: a photo from the dataset\n",
        "    :param monet: a monet image from the dataset\n",
        "    \"\"\"\n",
        "\n",
        "    false_monet, false_photo, g_loss = train_generator(photo, monet)\n",
        "    d_loss = train_discriminator(photo, monet, false_monet, false_photo)\n",
        "\n",
        "    return {'Generator loss': g_loss,\n",
        "            'Discriminator loss': d_loss}\n",
        "\n",
        "\n",
        "def train(start_epoch, epochs, augment=True):\n",
        "  \"\"\"\n",
        "  Full training function\n",
        "\n",
        "  start_epoch: epoch from which to start training (only interesting when using \n",
        "    checkpoints)\n",
        "  epochs: number of epochs to train for\n",
        "  augment: augmentation flag - whether to augment images using the \n",
        "    ImageDataGenerator or not\n",
        "  \"\"\"\n",
        "\n",
        "  # If a checkpoint is available, load it\n",
        "  try:\n",
        "    checkpoint.restore(manager.latest_checkpoint)\n",
        "    print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
        "  except:\n",
        "    print(\"Initializing from scratch.\")\n",
        "\n",
        "  generator_losses = []\n",
        "  discriminator_losses = []\n",
        "\n",
        "  # Create ImageDataGenerators based on the augment-flag. If augment is set to \n",
        "  # true, allow slight image augmentations like rotation, width and height \n",
        "  # shift, horizontal flip, etc.\n",
        "  if augment:\n",
        "    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        rotation_range=0.2,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        brightness_range=(0.8, 1.2),\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='reflect',\n",
        "    )\n",
        "  else:\n",
        "    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "\n",
        "  # Create data flow using both datasets (photo and monet) and given batch size\n",
        "  train_generator_photo = train_datagen.flow_from_directory(\n",
        "      'data/photo',\n",
        "      batch_size=BATCH_SIZE,\n",
        "      shuffle=True\n",
        "  )\n",
        "\n",
        "  train_generator_monet = train_datagen.flow_from_directory(\n",
        "      'data/monet',\n",
        "      batch_size=BATCH_SIZE,\n",
        "      shuffle=True\n",
        "  )\n",
        "\n",
        "  generator_losses_average = []\n",
        "  discriminator_losses_average = []\n",
        "\n",
        "  # Train for a given number of epochs\n",
        "  for epoch in np.arange(start_epoch, epochs):\n",
        "\n",
        "    print('--- EPOCH ' + str(epoch) + ' ---')\n",
        "    starttime = t.time()\n",
        "    generator_losses = []\n",
        "    discriminator_losses = []\n",
        "\n",
        "    # Every 10th epoch save the generated images to get the  fip-score over time\n",
        "    if epoch % 10 == 0:\n",
        "      save_gen = True\n",
        "      photo_path = 'fip-images/generated_photos_epoch' + str(epoch)\n",
        "      monet_path = 'fip-images/generated_monets_epoch' + str(epoch)\n",
        "    else:\n",
        "      save_gen = False\n",
        "\n",
        "    for i, ((p, p_l), (m, m_l)) in enumerate(zip(train_generator_photo, \n",
        "                                                 train_generator_monet)):\n",
        "\n",
        "      # Train the network and keep track of the losses\n",
        "      losses = train_single_step(norm(p), norm(m))\n",
        "      generator_losses.append(losses['Generator loss'])\n",
        "      discriminator_losses.append(losses['Discriminator loss'])\n",
        "\n",
        "      # ---- UNCOMMENT IF INTERESTED IN STORING IMAGES FOR POTENTIAL FID \n",
        "      # TRACKING ----\n",
        "      # Save the generated photos and monets\n",
        "      # if save_gen:\n",
        "      #   if not os.path.exists('fip-images'):\n",
        "      #       os.makedirs('fip-images')\n",
        "\n",
        "      #   # Save the generated photos\n",
        "      #   if not os.path.exists(photo_path):\n",
        "      #       os.makedirs(photo_path)\n",
        "\n",
        "      #   id2 = GEN_M2P(norm(m), training=False)\n",
        "      #   for idx in range(BATCH_SIZE):\n",
        "      #     img_photo = post(id2[idx])\n",
        "      #     cv2.imwrite(photo_path + '/' + str((i*BATCH_SIZE)+idx) + '.png', \n",
        "      #                 cv2.cvtColor(np.array(img_photo * 255).astype(np.uint8), \n",
        "      #                              cv2.COLOR_RGB2BGR))\n",
        "\n",
        "      #   # Save the generated Monets\n",
        "      #   if not os.path.exists(monet_path):\n",
        "      #       os.makedirs(monet_path)\n",
        "\n",
        "      #   id = GEN_P2M(norm(p), training=False)\n",
        "      #   for idx in range(BATCH_SIZE):\n",
        "      #     img_monet = post(id[idx])\n",
        "      #     cv2.imwrite(monet_path + '/' + str((i*BATCH_SIZE)+idx) + '.png', \n",
        "      #                 cv2.cvtColor(np.array(img_monet * 255).astype(np.uint8), \n",
        "      #                              cv2.COLOR_RGB2BGR))\n",
        "\n",
        "\n",
        "      # Save an image every 10 steps to track the progress using visual results\n",
        "      # (ignore the very first iteration per epoch as it nearly corresponds to \n",
        "      # the last step of the previous epoch)\n",
        "      if i % 10 == 0 and i != 0:\n",
        "\n",
        "          if not os.path.exists('images'):\n",
        "            os.makedirs('images')\n",
        "\n",
        "          # Save original photograph and generated monet\n",
        "          fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "          id = GEN_P2M(norm(p), training=False)\n",
        "          img_monet = post(id[0])\n",
        "          ax[0].imshow(p[0] / 255.)\n",
        "          ax[0].axis('off')\n",
        "          ax[1].imshow(img_monet)\n",
        "          ax[1].axis('off')\n",
        "          plt.savefig('images/' + str(epoch) + '_' + str(i) + '_photo2monet.png')\n",
        "          plt.close(fig)\n",
        "\n",
        "          # Save original monet and generated photograph\n",
        "          fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "          id2 = GEN_M2P(norm(m), training=False)\n",
        "          img_photo = post(id2[0])\n",
        "          ax[0].imshow(m[0] / 255.)\n",
        "          ax[0].axis('off')\n",
        "          ax[1].imshow(img_photo)\n",
        "          ax[1].axis('off')\n",
        "          plt.savefig('images/' + str(epoch) + '_' + str(i) + '_monet2photo.png')\n",
        "          plt.close(fig)\n",
        "\n",
        "          # Also create an average of the last 10 steps\n",
        "          generator_losses_average.append(np.mean(np.array(generator_losses)))\n",
        "          discriminator_losses_average.append(np.mean(np.array(discriminator_losses)))\n",
        "          generator_losses = []\n",
        "          discriminator_losses = []\n",
        "      \n",
        "      # break after a specific number of steps per epoch (tf image data \n",
        "      # generator will generate an infinite amount of images without break) and \n",
        "      # save the checkpoint - plot the current losses to track the progress\n",
        "      if i >= ((300 / BATCH_SIZE) - 1):\n",
        "        save_path = manager.save()\n",
        "        print('... finished after ' + str(t.time() - starttime) + ' seconds')\n",
        "        _, ax = plt.subplots(1, 2, figsize=(20, 4))\n",
        "        ax[0].plot(generator_losses_average, 'blue')\n",
        "        ax[0].set_title('Generator loss')\n",
        "        ax[1].plot(discriminator_losses_average, 'orange')\n",
        "        ax[1].set_title('Discriminator loss')\n",
        "        plt.show()\n",
        "        break\n",
        "\n",
        "  return generator_losses_average, discriminator_losses_average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYdlV3PWtEDO"
      },
      "outputs": [],
      "source": [
        "# Learning rate decay\n",
        "\n",
        "class LinearDecay(schedules.LearningRateSchedule):\n",
        "    # if `step` < `step_decay`: use fixed learning rate\n",
        "    # else: linearly decay the learning rate to zero\n",
        "\n",
        "    def __init__(self, initial_learning_rate, total_steps, step_decay):\n",
        "        super(LinearDecay, self).__init__()\n",
        "        self._initial_learning_rate = initial_learning_rate\n",
        "        self._steps = total_steps\n",
        "        self._step_decay = step_decay\n",
        "        self.current_learning_rate = tf.Variable(initial_value=initial_learning_rate, trainable=False, dtype=tf.float32)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        self.current_learning_rate.assign(tf.cond(\n",
        "            step >= self._step_decay,\n",
        "            true_fn=lambda: self._initial_learning_rate * (1 - 1 / (self._steps - self._step_decay) * (step - self._step_decay)),\n",
        "            false_fn=lambda: self._initial_learning_rate\n",
        "        ))\n",
        "        print(self.current_learning_rate)\n",
        "        return self.current_learning_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Initial training for 200 epochs"
      ],
      "metadata": {
        "id": "ChfJpREqejeG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxNKwX0xkkDk",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "IMG_HEIGHT = 256\n",
        "IMG_WIDTH = 256\n",
        "\n",
        "# Initialize networks\n",
        "GEN_P2M = ResnetGenerator()\n",
        "GEN_M2P = ResnetGenerator()\n",
        "DISC_P = Discriminator()\n",
        "DISC_M = Discriminator()\n",
        "\n",
        "# Set number of epochs to train for\n",
        "EPOCHS = 200\n",
        "\n",
        "# Set learning rate scheduler\n",
        "LR_INITIAL = 2e-4\n",
        "LR_G = LinearDecay(LR_INITIAL, \n",
        "                   EPOCHS * (300 / BATCH_SIZE), \n",
        "                   EPOCHS / 2 * (300 / BATCH_SIZE))\n",
        "LR_D = LinearDecay(LR_INITIAL, \n",
        "                   EPOCHS * (300 / BATCH_SIZE), \n",
        "                   EPOCHS / 2 * (300 / BATCH_SIZE))\n",
        "\n",
        "# Set optimizers\n",
        "GEN_ADAM = tf.keras.optimizers.Adam(learning_rate=LR_G, \n",
        "                                    beta_1=0.5, \n",
        "                                    beta_2=0.999)\n",
        "DISC_ADAM = tf.keras.optimizers.Adam(learning_rate=LR_D, \n",
        "                                     beta_1=0.5, \n",
        "                                     beta_2=0.999)\n",
        "\n",
        "# Define checkpoint storage place\n",
        "checkpoint = tf.train.Checkpoint(**dict(GEN_P2M=GEN_P2M, \n",
        "                                        GEN_M2P=GEN_M2P, \n",
        "                                        DISC_P=DISC_P, \n",
        "                                        DISC_M=DISC_M))\n",
        "manager = tf.train.CheckpointManager(checkpoint, \n",
        "                                     './tf_checkpoints', \n",
        "                                     max_to_keep=3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "START_EPOCH = 0\n",
        "gen_loss, disc_loss = train(START_EPOCH, EPOCHS, augment=True)"
      ],
      "metadata": {
        "id": "N1LR_e-beicj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Retraining without data augmentation for 100 epochs"
      ],
      "metadata": {
        "id": "kp5gSh0lel4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize networks\n",
        "GEN_P2M = ResnetGenerator()\n",
        "GEN_M2P = ResnetGenerator()\n",
        "DISC_P = Discriminator()\n",
        "DISC_M = Discriminator()\n",
        "\n",
        "# Set number of epochs to train for\n",
        "EPOCHS = 100\n",
        "\n",
        "# Set learning rate scheduler\n",
        "LR_INITIAL = 2e-5\n",
        "LR_G = LinearDecay(LR_INITIAL, \n",
        "                   EPOCHS * (300 / BATCH_SIZE), \n",
        "                   EPOCHS / 2 * (300 / BATCH_SIZE))\n",
        "LR_D = LinearDecay(LR_INITIAL, \n",
        "                   EPOCHS * (300 / BATCH_SIZE), \n",
        "                   EPOCHS / 2 * (300 / BATCH_SIZE))\n",
        "\n",
        "# Set optimizers\n",
        "GEN_ADAM = tf.keras.optimizers.Adam(learning_rate=LR_G, \n",
        "                                    beta_1=0.5, \n",
        "                                    beta_2=0.999)\n",
        "DISC_ADAM = tf.keras.optimizers.Adam(learning_rate=LR_D, \n",
        "                                     beta_1=0.5, \n",
        "                                     beta_2=0.999)\n",
        "\n",
        "# Define checkpoint storage place\n",
        "checkpoint = tf.train.Checkpoint(**dict(GEN_P2M=GEN_P2M, \n",
        "                                        GEN_M2P=GEN_M2P, \n",
        "                                        DISC_P=DISC_P, \n",
        "                                        DISC_M=DISC_M))\n",
        "manager = tf.train.CheckpointManager(checkpoint, \n",
        "                                     './tf_checkpoints', \n",
        "                                     max_to_keep=3)"
      ],
      "metadata": {
        "id": "_U4mu_Cpedwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44-oER71kyA5",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "START_EPOCH = 0\n",
        "gen_loss, disc_loss = train(START_EPOCH, EPOCHS, augment=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvRcsLjMQsL9"
      },
      "source": [
        "## 4 Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJj1JzgYpLSX"
      },
      "outputs": [],
      "source": [
        "def load_data_from_dir(data_dir, subset):\n",
        "    \"\"\"\n",
        "    Loads and preprocesses images from a given directory.\n",
        "\n",
        "    data_dir: Name of the directory\n",
        "    \"\"\"\n",
        "\n",
        "    # Load data\n",
        "    dataset = tf.keras.utils.image_dataset_from_directory(data_dir,\n",
        "                                                          # validation_split=0.2,\n",
        "                                                          # subset=subset,\n",
        "                                                          seed=123,\n",
        "                                                          shuffle=True,\n",
        "                                                          image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                          batch_size=1)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xEAAG9-R4Ve"
      },
      "outputs": [],
      "source": [
        "def create_and_save_results(test_data, dir_name, p2m=True):\n",
        "  \"\"\"\n",
        "  Generates domain transfer from the original to the target domain between monet \n",
        "  images and photographs.\n",
        "  \n",
        "  test_data: data as tensors\n",
        "  dir_name: name of dir where to save the generated samples\n",
        "  p2m: whether the transfer direction is photo to monet, or the opposite \n",
        "    direction\n",
        "  \"\"\"\n",
        "\n",
        "  if not os.path.exists(dir_name):\n",
        "    os.makedirs(dir_name)\n",
        "\n",
        "  if not os.path.exists(dir_name + '/generated'):\n",
        "    os.makedirs(dir_name + '/generated')\n",
        "\n",
        "  if not os.path.exists(dir_name + '/original'):\n",
        "    os.makedirs(dir_name + '/original')\n",
        "  \n",
        "  for i, img in enumerate(test_data):\n",
        "    \n",
        "    # Do not use more than a maximal number of images\n",
        "    # if i > 1000:\n",
        "    #   break\n",
        "\n",
        "    # Generate either photo or Monet painting and postprocess the generated \n",
        "    # image to the correct range (0 to 1) for saving\n",
        "    if p2m:\n",
        "      generated = GEN_P2M(norm(img[0]), training=False)\n",
        "    else:\n",
        "      generated = GEN_M2P(norm(img[0]), training=False)\n",
        "    generated_postprocessed = post(generated)\n",
        "\n",
        "    # Save images:\n",
        "    cv2.imwrite(dir_name + '/original/' + str(i) + '.png', \n",
        "                cv2.cvtColor(np.array(img[0][0]).astype(np.uint8), \n",
        "                             cv2.COLOR_RGB2BGR))\n",
        "    cv2.imwrite(dir_name + '/generated/' + str(i) + '.png', \n",
        "                cv2.cvtColor(np.array(generated_postprocessed[0] * 255).astype(np.uint8), \n",
        "                             cv2.COLOR_RGB2BGR))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the original data\n",
        "test_monets = load_data_from_dir('data/monet', subset='validation')\n",
        "test_photos = load_data_from_dir('data/photo', subset='validation')"
      ],
      "metadata": {
        "id": "D3q1vODGe2iY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Mcf_z0-yuSA"
      },
      "outputs": [],
      "source": [
        "# Photo to Monet\n",
        "create_and_save_results(test_photos, 'Photo2Monet', p2m=True)\n",
        "\n",
        "# Monet to photo\n",
        "create_and_save_results(test_monets, 'Monet2Photo', p2m=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1Td8omzq2pDE"
      },
      "outputs": [],
      "source": [
        "# ---- UNCOMMENT TO UPLOAD GENERATED IMAGES TO DRIVE AS ZIP FILE ----\n",
        "#!zip -r '/content/gdrive/MyDrive/Photo2Monet-2022-08-17.zip' '/content/Photo2Monet'\n",
        "#!zip -r '/content/gdrive/MyDrive/Monet2Photo-2022-08-17.zip' '/content/Monet2Photo'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Od1BgffFm_B"
      },
      "source": [
        "## 5 Evaluation\n",
        "The quantitative evaluation for the present work is performed using the Fr√©chet Inception Disctance (FID), which is commonly used to assess generative adversarial networks. The following code blocks calculate the FIDs between the original datasets (Monet vs. photographs), original vs. generated photographs, and original vs. generated Monets. Uses [this](https://github.com/mseitzer/pytorch-fid) FID calculation implementation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FID calculation of original datasets\n",
        "!python -m pytorch_fid '/content/Photo2Monet/original' '/content/Monet2Photo/original'"
      ],
      "metadata": {
        "id": "nX7Dn7hdliEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJ8WnVO5E6Cp"
      },
      "outputs": [],
      "source": [
        "# FID calculation of generated photographs\n",
        "!python -m pytorch_fid '/content/Photo2Monet/original' '/content/Monet2Photo/generated'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "910zEKvqE_4U"
      },
      "outputs": [],
      "source": [
        "# FID calculation of generated Monets\n",
        "!python -m pytorch_fid '/content/Monet2Photo/original' '/content/Photo2Monet/generated'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Reimplementation_CycleGAN.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}